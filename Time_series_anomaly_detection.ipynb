{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhdadizadeh/Time-series-data-anomaly-detection/blob/main/Time_series_anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J44h3JOcqDWQ",
        "outputId": "b4eee0ca-9e8c-4654-fa51-dd4710547ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JLYStWlgQpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52dfbd5-c34d-4275-e5de-6fb7d83fa416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| 0.6666666666666666 12.csv 1680\n",
            "| 0.8333333333333333 15.csv 1680\n",
            "| 1.0 13.csv 1680\n",
            "| 1.0 16.csv 1680\n",
            "| 0.14423752635277584 0.csv 128562\n",
            "| 0.09379247847757137 1.csv 129128\n",
            "| 1.0 18.csv 1680\n",
            "| 0.888888888888889 19.csv 1680\n",
            "| 1.0 17.csv 1680\n",
            "| 0.6666666666666666 11.csv 1680\n",
            "| 0.9696969696969697 14.csv 1680\n",
            "| 0.888888888888889 10.csv 1680\n",
            "| 1.0 34.csv 1680\n",
            "| 0.967741935483871 44.csv 1680\n",
            "| 0.6555760936537277 2.csv 146254\n",
            "| 0.6820512820512821 47.csv 8863\n",
            "| 0.888888888888889 23.csv 1680\n",
            "| 0.1935483870967742 45.csv 1680\n",
            "| 0.00045063243932001116 46.csv 128679\n",
            "| 0.19878603945371773 4.csv 147668\n",
            "| 0.018691588785046728 41.csv 1680\n",
            "| 0.0 42.csv 1680\n",
            "| 1.0 20.csv 1680\n",
            "| 0.47058823529411764 40.csv 1680\n",
            "| 1.0 25.csv 1680\n",
            "| 0.3636363636363636 35.csv 1680\n",
            "| 0.053231939163498096 39.csv 1680\n",
            "| 1.0 32.csv 1680\n",
            "| 0.7272727272727273 29.csv 1680\n",
            "| 0.5333333333333333 27.csv 1680\n",
            "| 1.0 21.csv 1680\n",
            "| 0.028571428571428574 36.csv 1680\n",
            "| 0.5 31.csv 1680\n",
            "| 0.6226893802102211 3.csv 146253\n",
            "| 0.888888888888889 24.csv 1680\n",
            "| 0.8461538461538461 38.csv 1680\n",
            "| 0.2857142857142857 30.csv 1680\n",
            "| 0.5 28.csv 1680\n",
            "| 1.0 37.csv 1680\n",
            "| 0.761904761904762 22.csv 1680\n",
            "| 0.7058823529411764 26.csv 1680\n",
            "| 0.4 33.csv 1680\n",
            "| 0.375 43.csv 1680\n",
            "| 0.8181818181818181 55.csv 1461\n",
            "| 0.08333333333333333 58.csv 1435\n",
            "| 0.019801980198019806 57.csv 1427\n",
            "| 0.625 56.csv 1461\n",
            "| 0.4 59.csv 1427\n",
            "| 0.09096209912536443 49.csv 147024\n",
            "| 0.6666666666666666 54.csv 1427\n",
            "| 0.7441860465116279 52.csv 1439\n",
            "| 0.407557354925776 5.csv 146228\n",
            "| 0.7280334728033472 48.csv 146255\n",
            "| 0.5588235294117647 53.csv 1420\n",
            "| 0.7527472527472527 51.csv 1424\n",
            "| 0.5 50.csv 1461\n",
            "| 1.0 72.csv 1421\n",
            "| 1.0 7.csv 1680\n",
            "| 1.0 80.csv 1421\n",
            "| 1.0 70.csv 1421\n",
            "| 1.0 66.csv 1421\n",
            "| 1.0 67.csv 1421\n",
            "| 1.0 79.csv 1421\n",
            "| 1.0 77.csv 1421\n",
            "| 1.0 76.csv 1421\n",
            "| 0.358974358974359 61.csv 741\n",
            "| 0.3994038748137109 6.csv 129453\n",
            "| 1.0 74.csv 1421\n",
            "| 1.0 65.csv 1421\n",
            "| 1.0 73.csv 1421\n",
            "| 1.0 78.csv 1421\n",
            "| 1.0 63.csv 1421\n",
            "| 0.8695652173913043 8.csv 1680\n",
            "| 0.5555555555555556 60.csv 1432\n",
            "| 0.6399999999999999 62.csv 1461\n",
            "| 1.0 68.csv 1421\n",
            "| 1.0 69.csv 1421\n",
            "| 1.0 9.csv 1680\n",
            "| 1.0 75.csv 1421\n",
            "| 1.0 71.csv 1421\n",
            "| 1.0 64.csv 1421\n",
            "    F1 score\n",
            "12  0.666667\n",
            "15  0.833333\n",
            "13  1.000000\n",
            "16  1.000000\n",
            "0   0.144238\n",
            "..       ...\n",
            "69  1.000000\n",
            "9   1.000000\n",
            "75  1.000000\n",
            "71  1.000000\n",
            "64  1.000000\n",
            "\n",
            "[81 rows x 1 columns]\n",
            "mean F1 score    0.691846\n",
            "dtype: float64\n",
            "median F1 score    0.761905\n",
            "dtype: float64\n",
            "F1 score\n",
            "1.000000    30\n",
            "0.888889     4\n",
            "0.500000     3\n",
            "0.666667     3\n",
            "0.400000     2\n",
            "0.728033     1\n",
            "0.625000     1\n",
            "0.640000     1\n",
            "0.655576     1\n",
            "0.682051     1\n",
            "0.705882     1\n",
            "0.727273     1\n",
            "0.744186     1\n",
            "0.558824     1\n",
            "0.752747     1\n",
            "0.761905     1\n",
            "0.818182     1\n",
            "0.833333     1\n",
            "0.846154     1\n",
            "0.869565     1\n",
            "0.967742     1\n",
            "0.969697     1\n",
            "0.622689     1\n",
            "0.000000     1\n",
            "0.000451     1\n",
            "0.533333     1\n",
            "0.018692     1\n",
            "0.019802     1\n",
            "0.028571     1\n",
            "0.053232     1\n",
            "0.083333     1\n",
            "0.090962     1\n",
            "0.093792     1\n",
            "0.144238     1\n",
            "0.193548     1\n",
            "0.198786     1\n",
            "0.285714     1\n",
            "0.358974     1\n",
            "0.363636     1\n",
            "0.375000     1\n",
            "0.399404     1\n",
            "0.407557     1\n",
            "0.470588     1\n",
            "0.555556     1\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# trend neighbor residual product test\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "# from dateutil.parser import parse\n",
        "import math\n",
        "\n",
        "#++++++++++++++\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#--------------\n",
        "\n",
        "# input_path = os.environ.get('DATA_104_PATH') + '/test_data'\n",
        "# output_path = '/output'\n",
        "\n",
        "#+++++++++++++++++\n",
        "output_path = '//drive//MyDrive//data_days//output'\n",
        "input_path = \"//drive//MyDrive//data_days//dataset//\"\n",
        "#-----------------\n",
        "\n",
        "\n",
        "  #+++++++++++++ \n",
        "class mh_plot:\n",
        "\n",
        "  def __call__(self,df, x_axis, y_axis, hue_by, start=0, end=100000):\n",
        "      plt.figure(figsize=(30, 5))\n",
        "      sns.scatterplot(x=x_axis, y=y_axis, hue=hue_by, data=df[start:end], legend='full')\n",
        "      plt.show()\n",
        "  #--------------\n",
        "\n",
        "class TimeSeriesAnomalyDetector:\n",
        "\n",
        "    def __call__(self, df):\n",
        "\n",
        "        main_df = df.copy()\n",
        "        main_df['value'] += abs(main_df['value'].min()) + 1\n",
        "        main_df['predict_1']= None\n",
        "        main_df['predict_2']= None\n",
        "        main_df['label'] = None\n",
        "\n",
        "        model_1_ = self.regression_outlier_model(main_df)\n",
        "        model_2_predict = self.neighborhood_distance(main_df)\n",
        "\n",
        "        main_df['predict_1'] = self.regression_outlier_model(main_df)\n",
        "        main_df['predict_2'] = self.neighborhood_distance(main_df)\n",
        "        main_df['predict_3'] = self.decomposition_model(main_df)\n",
        "\n",
        "        main_df['label'] = main_df['predict_1'] | main_df['predict_2'] | main_df['predict_3'] \n",
        "\n",
        "        main_df['label'] = main_df['label'].replace(False, 0)\n",
        "        main_df['label'] = main_df['label'].replace(True, 1)     \n",
        "\n",
        "        \n",
        "        return main_df[['timestamp', 'value', 'label']]\n",
        "\n",
        "\n",
        "    def linear_detect_outliers(self, input_values):\n",
        "\n",
        "        values = input_values.copy()\n",
        "\n",
        "        Q1 = np.percentile(values, 25, interpolation = 'midpoint')  \n",
        "        Q3 = np.percentile(values, 75, interpolation = 'midpoint')\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        upper = values >= (Q3+1.5*IQR)\n",
        "        lower = values <= (Q1-1.5*IQR)\n",
        "\n",
        "        outliers = upper | lower \n",
        "\n",
        "        return outliers\n",
        "\n",
        "\n",
        "    def decompose_detect_outliers(self, input_values):\n",
        "\n",
        "        values = input_values.copy()\n",
        "\n",
        "        Q1 = np.percentile(values, 5, interpolation = 'midpoint')  \n",
        "        Q3 = np.percentile(values, 95, interpolation = 'midpoint')\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        upper = values >= (Q3+1.5*IQR)\n",
        "        lower = values <= (Q1-1.5*IQR)\n",
        "\n",
        "        outliers = upper | lower \n",
        "\n",
        "        return outliers\n",
        "      \n",
        "    \n",
        "    def neighbor_detect_outliers(self, input_values):\n",
        "      \n",
        "        values = input_values.copy()\n",
        "\n",
        "        Q1 = np.percentile(values, 3, interpolation = 'midpoint')  \n",
        "        Q3 = np.percentile(values, 97, interpolation = 'midpoint')\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        upper = values >= (Q3+1.5*IQR)\n",
        "        lower = values <= (Q1-1.5*IQR)\n",
        "\n",
        "        outliers = upper | lower \n",
        "\n",
        "        return outliers\n",
        "        \n",
        "\n",
        "    def neighborhood_distance(self, input_df):\n",
        "        \n",
        "        df = input_df.copy()\n",
        "\n",
        "        df['left_dist'] = (df['value'] - df['value'].shift(1)).abs()\n",
        "        df['right_dist'] = (df['value'] - df['value'].shift(-1)).abs()\n",
        "        df['neighbor_dist'] = df[['left_dist', 'right_dist']].min(axis=1)\n",
        "        df['predict'] = self.neighbor_detect_outliers(df['neighbor_dist'])\n",
        "        \n",
        "        return df['predict']\n",
        "\n",
        "    def linear_regression(self, input_df):\n",
        "\n",
        "        df = input_df.copy()\n",
        "        regr = linear_model.LinearRegression()\n",
        "        train_x = np.array(df[['timestamp']])\n",
        "        train_y =  np.array(df[['value']])\n",
        "        regr.fit (train_x, train_y)\n",
        "        regression_line = (regr.coef_[0][0]*train_x + regr.intercept_[0])\n",
        "        print('line: ',regression_line)\n",
        "\n",
        "        return regression_line\n",
        "\n",
        "\n",
        "    def Polynomial_line(self, input_df):\n",
        "\n",
        "        df = input_df.copy()\n",
        "\n",
        "        train_x = np.array(df[['timestamp']])\n",
        "        train_y =  np.array(df[['value']])\n",
        "        \n",
        "        poly = PolynomialFeatures(degree=2)\n",
        "        train_x_poly = poly.fit_transform(train_x)\n",
        "\n",
        "        regr = linear_model.LinearRegression()\n",
        "        train_y_ = regr.fit(train_x_poly, train_y)\n",
        "        XX = np.arange(0.0, len(df.index))\n",
        "        regression_line = (regr.intercept_[0]+ regr.coef_[0][1]*XX+ regr.coef_[0][2]*np.power(XX, 2))\n",
        "        line_size = len(df.index)\n",
        "        regression_line = np.reshape(regression_line, (line_size, 1))\n",
        "\n",
        "        return regression_line\n",
        "\n",
        "\n",
        "    def regression_outlier_model(self, input_df):\n",
        "    \n",
        "        df = input_df.copy()\n",
        "        no_slope_value = df[['value']] - self.Polynomial_line(df)\n",
        "        return list(self.linear_detect_outliers(no_slope_value['value']))\n",
        "\n",
        "\n",
        "    def trend_line(self, input_df):\n",
        "\n",
        "        initial_df = input_df.copy()\n",
        "        temp_df = input_df.copy()\n",
        "        df = initial_df[['timestamp', 'value']]\n",
        "        df = df.set_index('timestamp')\n",
        "\n",
        "        additive_decomposition = seasonal_decompose(df, model='additive', period=30)\n",
        "\n",
        "        trend = additive_decomposition.trend\n",
        "        trend[0:30] = trend.iloc[31]\n",
        "        trend[-30:0] = trend.iloc[-31]\n",
        "        trend = np.array(trend)\n",
        "        line_size = len(trend)\n",
        "        trend = np.reshape(trend, (line_size, 1))\n",
        "      \n",
        "        return trend\n",
        "\n",
        "\n",
        "    def decomposition_model(self, input_df):\n",
        "\n",
        "        initial_df = input_df.copy()\n",
        "        temp_df = input_df.copy()\n",
        "        df = initial_df[['timestamp', 'value']]\n",
        "        df = df.set_index('timestamp')\n",
        "\n",
        "        additive_decomposition = seasonal_decompose(df, model='additive', period=30)\n",
        "\n",
        "        trend = additive_decomposition.trend\n",
        "        seasonal = additive_decomposition.seasonal\n",
        "        residual = additive_decomposition.resid\n",
        "        residual = list(residual.fillna(residual.mean()))\n",
        "        temp_df['value'] = residual\n",
        "\n",
        "        return  self.decompose_detect_outliers(residual) \n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "anomaly_detector = TimeSeriesAnomalyDetector()\n",
        "#+++++++++++++\n",
        "plotter = mh_plot()\n",
        "#--------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #+++++++++++++ \n",
        "    score_dict = {}\n",
        "    score_list = []\n",
        "    #--------------\n",
        "    filename_list = ['27.csv', '28.csv']\n",
        "    # os.listdir(input_path)\n",
        "    for filename in os.listdir(input_path):\n",
        "        input_df = pd.read_csv(os.path.join(input_path, filename))\n",
        "        #+ print(filename, len(input_df))\n",
        "        result = anomaly_detector(input_df)\n",
        "        #+++++++++++++\n",
        "        # print('orginal')\n",
        "        # plotter(df = input_df, x_axis = 'timestamp', y_axis = 'value', hue_by = 'label')\n",
        "        # print('predict')\n",
        "        # plotter(df = result, x_axis = 'timestamp', y_axis = 'value', hue_by = 'label')\n",
        "\n",
        "        # print(result)\n",
        "\n",
        "        # print(input_df['label'].value_counts())\n",
        "        # print(result['label'].value_counts())\n",
        "        f1 = f1_score(input_df['label'], result['label'])\n",
        "        score_list.append(f1)\n",
        "        print('|', f1, end=' ')\n",
        "        print(filename, len(input_df))\n",
        "        #--------------\n",
        "        \n",
        "        #+ result.to_csv(os.path.join(output_path, filename))\n",
        "        #- print(f'item {filename} processed.')\n",
        "\n",
        "    #+++++++++++++\n",
        "    # print(result)\n",
        "    score_index = [i.split('.')[0] for i in os.listdir(input_path)]\n",
        "    result = pd.DataFrame(data= score_list, index= score_index, columns=['F1 score'])\n",
        "    print(result)\n",
        "    print('mean',result.mean())\n",
        "    print('median', result.median())\n",
        "    print(result.value_counts())\n",
        "    #--------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0, 1, 2])"
      ],
      "metadata": {
        "id": "5Ni9iaKzLEUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjLbXclMLWtI",
        "outputId": "bd82137f-a40f-401d-f52c-e13fad4dd404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.reshape(a, (3, 1)) # C-like index ordering\n",
        "a"
      ],
      "metadata": {
        "id": "gEYI9-jLvLJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b8144b-e12a-4ead-ae93-1c16fc710805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [2]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybDDUnD7LhJV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSJfxPrSCYDFLhFjudwaTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}